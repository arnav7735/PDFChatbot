{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arnav\\anaconda3\\envs\\langchain\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation of environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGCHAIN_TRACING_V2: true\n",
      "LANGCHAIN_ENDPOINT: https://api.smith.langchain.com\n",
      "LANGCHAIN_API_KEY: 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\n",
      "LANGCHAIN_PROJECT: pr-new-starter-46\n",
      "GOOGLE_API_KEY: AIzaSyA8G3UEtOaguEcKbVJu-PSkhFzduu5BSBw\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Fetch environment variables from .env\n",
    "LANGCHAIN_TRACING_V2 = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "LANGCHAIN_ENDPOINT = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGCHAIN_PROJECT = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Optional print to verify\n",
    "print(\"LANGCHAIN_TRACING_V2:\", LANGCHAIN_TRACING_V2)\n",
    "print(\"LANGCHAIN_ENDPOINT:\", LANGCHAIN_ENDPOINT)\n",
    "print(\"LANGCHAIN_API_KEY:\", LANGCHAIN_API_KEY)\n",
    "print(\"LANGCHAIN_PROJECT:\", LANGCHAIN_PROJECT)\n",
    "print(\"GOOGLE_API_KEY:\", GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Gemini Model(or any model of your choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.4, google_api_key=GOOGLE_API_KEY, convert_system_message_to_human=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract text from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating chunks for batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=1000):\n",
    "    sentences = text.split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if current_length + len(sentence.split()) > chunk_size:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        current_chunk.append(sentence)\n",
    "        current_length += len(sentence.split())\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing these chunks in a FAISS DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(chunks):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')  # Using a smaller, efficient model\n",
    "    chunk_embeddings = model.encode(chunks)\n",
    "    index = faiss.IndexFlatL2(chunk_embeddings.shape[1])  # L2 distance for similarity\n",
    "    index.add(np.array(chunk_embeddings))\n",
    "    return index, chunk_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval Function from the FAISS DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query, index, chunks, chunk_embeddings, top_k=3):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    query_embedding = model.encode([query])\n",
    "    _, I = index.search(query_embedding, top_k)\n",
    "    return [chunks[i] for i in I[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post Retrieval Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(chat, retrieved_chunks, user_query):\n",
    "    # Combine retrieved chunks and user's query\n",
    "    context = \" \".join(retrieved_chunks)\n",
    "    message = [\n",
    "        SystemMessage(content=f\"Use the following context to answer: {context}\"),\n",
    "        HumanMessage(content=user_query)\n",
    "    ]\n",
    "    result = chat.invoke(message)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pdf_path = r\"C:\\Users\\Arnav\\Desktop\\msd\\attention_is_all_you_need.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_text(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arnav\\anaconda3\\envs\\langchain\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "index, chunk_embeddings = create_faiss_index(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arnav\\anaconda3\\envs\\langchain\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "user_query = \"In brief give me the major important topics being discussed in this pdf\"\n",
    "retrieved_chunks = retrieve_relevant_chunks(user_query, index, chunks, chunk_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arnav\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_google_genai\\chat_models.py:381: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper presents a new neural network architecture called the Transformer, which is based solely on attention mechanisms and dispenses with recurrence and convolutions. The Transformer is shown to be superior in quality to existing models on two machine translation tasks, while being more parallelizable and requiring significantly less time to train. The major important topics being discussed in this paper are:\n",
      "\n",
      "1. The Transformer architecture, which consists of an encoder and a decoder, each of which is composed of a stack of identical layers.\n",
      "\n",
      "2. The self-attention mechanism, which allows each position in the input or output sequence to attend to all other positions.\n",
      "\n",
      "3. The multi-head attention mechanism, which allows the model to jointly attend to information from different representation subspaces at different positions.\n",
      "\n",
      "4. The position-wise feed-forward network, which is applied to each position in the input or output sequence separately and identically.\n",
      "\n",
      "5. The training procedure for the Transformer, which uses a combination of supervised learning and reinforcement learning.\n",
      "\n",
      "6. The results of the Transformer on two machine translation tasks, which show that the Transformer achieves state-of-the-art results on both tasks.\n"
     ]
    }
   ],
   "source": [
    "response = answer_question(chat, retrieved_chunks, user_query)\n",
    "\n",
    "# Display the response\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
